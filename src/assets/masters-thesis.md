# Master's Thesis - CTML Berkeley

## Research in Mechanistic Interpretability for TMLE Neural Network Enhancement

### Overview

This research project was conducted at the Center for Targeted Machine Learning and Causal Inference (CTML) at UC Berkeley under the supervision of Professor Mark van der Laan.

The master thesis can be found at [here](https://arxiv.org/abs/2505.00555).

### Background

Targeted Minimum Loss-based Estimation (TMLE) is a semiparametric, efficient substitution estimator. This research explores the integration of mechanistic interpretability (mech interp) methods with TMLE frameworks using neural networks to enhance both data treatment and model decision interpretation.

### Research Focus

The research focused on:

- **Mechanistic Interpretability**: Developing methods to understand neural network decision-making processes within TMLE frameworks
- **Neural Network Enhancement**: Integrating deep learning architectures with traditional TMLE estimators
- **Data Treatment Innovation**: Creating new approaches for data preprocessing and feature engineering through interpretable neural components
- **Model Decision Interpretation**: Building tools to explain and validate neural network decisions in causal inference contexts

### Key Contributions

- Developed novel mechanistic interpretability techniques for TMLE-integrated neural networks
- Created interpretable neural architectures that maintain TMLE's theoretical guarantees
- Implemented new data treatment methodologies using interpretable deep learning components
- Advanced model explainability in causal inference through mechanistic understanding of neural network behavior

### Technical Stack

- **Python**: Primary language for neural network implementation and mechanistic interpretability
- **PyTorch/TensorFlow**: Deep learning frameworks for neural TMLE implementations
- **R**: Statistical computing and traditional TMLE validation
- **Mechanistic Interpretability**: Circuit analysis, feature visualization, activation patching
- **Causal Inference**: TMLE with neural network enhancements

### Publications and Presentations

Research findings on mechanistic interpretability applications in causal inference were presented at multiple conferences and are being prepared for publication in peer-reviewed journals.

### Location
UC Berkeley, California

### Duration
2023 - 2024

### Supervisor
Professor Mark van der Laan, Division of Biostatistics, UC Berkeley 